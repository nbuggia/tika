Cleaned build directory
{'slug': '2012-05-30-coq-au-vin', 'build_path': './build/articles/2012-05-30-coq-au-vin.html', 'category': '', 'url': '/articles/2012-05-30-coq-au-vin.html', 'date': datetime.datetime(2012, 5, 30, 0, 0), 'content_html': '<p>I really like the simplicity and deliciousness of this recipe. You can make it \nmore complicated, but I don’t know that makes it better. You can do this all \nin one go (4 hours), or you can split it into two days. Both are good, but \nfood always tastes best when it’s had some time together.</p>\n<p>You can pretty make this dish with anything. The core of this recipe is to slow \ncook something in wine, stock and aromatic veggies, then let it sit around for \na while. If you do that, it will taste great.</p>\n<h2>Ingredients</h2>\n<ul>\n<li>1 or 2 Chicken legs per person</li>\n<li>2 or 3 Slabs of bacon</li>\n<li>1 Onion</li>\n<li>2 Stalks Celery</li>\n<li>2 Carrots</li>\n<li>2 Handfuls of Mushrooms</li>\n<li>12 Stalks of Thyme</li>\n<li>1 Bay Leaf</li>\n<li>1 Box of Chicken Stock (maybe 500ml)</li>\n<li>750ml Red Wine (1 Bottle Pinon Noir)</li>\n<li>Optional: potatoes, parsnips, turnip, pretty much any veggie</li>\n<li>About a cup of Flour</li>\n<li>Two table spoons of tomato paste</li>\n<li>Two garlic cloves</li>\n</ul>\n<h2>Instructions</h2>\n<ul>\n<li>Coat the chicken thighs in Salt &amp; Pepper, and then flour. I use a pie plate \nbecause it is flat and wide. Don’t over think this.</li>\n<li>Pour the bottle of wine and the box of chicken stock in your dutch oven. Add \nthe two table spoons of tomato paste. Add two table spoons of olive oil.</li>\n<li>Sautee the bacon in a frying pan with a couple table-spoons of water. Once \nyou have them browned and have created enough bacon grease to fry things in, \npush the bacon aside and brown the following items in the pan (add butter if \nyou start running out of bacon grease): Flour-coated chicken, Mushrooms, Onions. </li>\n<li>Then put them in the dutch oven (including the bacon, which you might want to \ncut into smaller pieces first).</li>\n<li>Add the carrots, celery, Bay Leaf, Garlic and Thyme into the dutch oven (if \nyou are anal retentive, you can put the Thyme &amp; Bay Leaf in cheese cloth for \neasy removal later)</li>\n<li>Set the Dutch oven in your refrigerator and wait 1 day</li>\n<li>Next day… cook the dutch oven in your stove for 2.5 hours at 325 degrees \nfahrenheit</li>\n<li>Take it out and simmer it on your stove top (without the lid) for another \nhour or so to reduce the liquid down by 30-40%.</li>\n<li>Then you can thicken the juices by adding flour. The anal retentive will \nwant to gently strain out the liquid and add flour to it before recombining. \nBut I usually just pour the flour in with everything and poke/ stir until the \nflour dissolves.</li>\n<li>Eat. Drink. Be Merry!</li>\n</ul>', 'title': 'Coq au Vin'}
----------------
{'slug': '2016-07-25-chana-masala', 'build_path': './build/articles/Cooking/2016-07-25-chana-masala.html', 'category': 'Cooking', 'url': '/articles/Cooking/2016-07-25-chana-masala.html', 'date': datetime.datetime(2016, 7, 25, 0, 0), 'content_html': '<p>Chana Masala is a tomato-y, chickpea-y soupy dish that goes well with rice, naan or roasted chicken. </p>\n<h3>Steps</h3>\n<ol>\n<li>Pre-chop all the veggies</li>\n<li>Heat a large pot over medium-high heat. When hot add 2 Tbsp Canola oil. </li>\n<li>Add onions, let brown for 5 min. Then add garlic and ginger. Continue to brown for 5 more minutes. Mixture should be golden brown.</li>\n<li>Add all spices (except Garam Masala). Let cook for 1 minute.<ul>\n<li>Halved cinnamon stick (one half of this seemed sufficient)</li>\n<li>1 tsp cayenne pepper</li>\n<li>1 tsp cumin</li>\n<li>1 tsp mustard seeds</li>\n<li>1 tsp turmeric</li>\n</ul>\n</li>\n<li>Add tomatoes and 2 cups water. Bring to a boil and scrap the bottom of the pan to ensure all the browning bits are no longer stuck. Add chick peas and stir the mixture. Let simmer for 15 minutes partially covered.</li>\n<li>Add Garam Masala and 1 Tbsp lime juice. Let simmer for 15 more minutes. You can break up some of the chickpeas to add thickness to the broth.</li>\n<li>Season with salt to tast. Remove cinnamon stick and serve. Garnish with Cilantro.</li>\n</ol>\n<h3>Other versions to investigate</h3>\n<p>http://minimalistbaker.com/easy-chana-masala/\nhttps://smittenkitchen.com/2010/02/chana-masala/</p>', 'title': 'Chana Masala', 'ingredients': '2 Tbsp Canola oil; 1 yellow onion, finely chopped; 2 cloves garlic, minced; 1 inch fresh Ginger, peeled and grated; 1 cinnamon stick, halved; 1 tsp cayenne pepper; 1 tsp ground cumin; 1 tsp brown mustard seeds; 1 tsp ground turmeric; 1 can diced tomatoes (28 oz), drained; 2 tsp sugar; Salt; 2 cups rinsed and drained canned chipeas; 1 Tbsp garam masala; 1 Tbsp fresh lime juice; 1 cup fresh cilantro, rough chopped;', 'prep': '30 min', 'cook': '1 hour'}
----------------
{'slug': '2018-08-27-granola', 'build_path': './build/articles/Cooking/2018-08-27-granola.html', 'category': 'Cooking', 'url': '/articles/Cooking/2018-08-27-granola.html', 'date': datetime.datetime(2018, 8, 27, 0, 0), 'content_html': '<p>Combine everything in a big bowl. Cook at 250 degrees. Stir every 15 minutes.</p>\n<h1>August 18, 2018</h1>\n<p>3 cups rolled oats\n1 cup walnuts\n1 cup cashews\n1/2 cup almonds\n1.5 cups raisins\n1/4 cup brown sugar\n1/4 cup maple syrup\n1/4 cup olive oil\n3/4 teaspoon salt</p>', 'title': 'Granola', 'ingredients': '3 cups rolled oats; 1 cup walnuts; 1 cup cashews; 1/2 cup almonds; 1.5 cups raisins; 1/4 cup brown sugar; 1/4 cup olive oil; 3/4 teaspoon salt;', 'prep': '20 min', 'cook': '1 hour 15 min'}
----------------
{'slug': '2016-07-07-applesauce-spiced-rum-cake', 'build_path': './build/articles/Cooking/2016-07-07-applesauce-spiced-rum-cake.html', 'category': 'Cooking', 'url': '/articles/Cooking/2016-07-07-applesauce-spiced-rum-cake.html', 'date': datetime.datetime(2016, 7, 7, 0, 0), 'content_html': '<p>Best recipe ever...</p>', 'title': 'Applesauce Spiced Rum Cake', 'ingredients': '1 pinch of love; 1 tbs of heart;'}
----------------
{'slug': '2018-06-19-mushroom-rissotto', 'build_path': './build/articles/Cooking/2018-06-19-mushroom-rissotto.html', 'category': 'Cooking', 'url': '/articles/Cooking/2018-06-19-mushroom-rissotto.html', 'date': datetime.datetime(2018, 6, 19, 0, 0), 'content_html': '<p>Mushroom Risotto (these risotto recipes feed 2 very comfortably) \nArborio rice (the “risotto” rice. You can find at TJs in a red box or in most \ngrocery stores, where it can be quite a bit more expensive. You’ll need about 1 \nand ½ cups rice, if you are of the measuring type.)</p>\n<p>Celery (2 or 3 stalks)\n1 medium onion\n2 garlic cloves, chopped\n2 lemons \nA handful of parsley\n1 cup white wine\nOlive oil</p>\n<p>Parmesan, pecorino or asiago cheese to finish your risotto\n4 cups assorted mushrooms of your choice (dried porcini mushrooms (soak in water \nfirst/use water for broth), cremini mushrooms and portobello all at TJs, oyster \nmushrooms, enoki or something different from farmer\'s market</p>\n<p>To prep the mushrooms:\nSlice about 4 medium (or two large) portobello mushrooms &amp; half a package of \ncremini mushrooms, and sauté with garlic.  Set aside and repeat with any oyster \nor enoki mushrooms (their cooking time is shorter).  Combine the mushrooms when \ncooked, season with salt and a squeeze of lemon to taste.  Maybe too you\'d like \nto add some fresh herbs from the garden.\n(You can also consider making a porcini mushroom stock by placing about half a \npacket of (bashed) dried porcinis with 5 cups or so of water in a saucepan on \nmedium heat.  When it is a lovely brown color, and smells yummy, it is done.)</p>\n<p>To cook the risotto:\nPractice your knife skills (yeah!) and dice the celery and onions, small.  Slow \nand steady wins the race... make nice, even cubes.  Sweat the celery and onion \nin a wide, heavy-bottomed pot with a bit of olive oil and/or butter (1-2 \ntablespoons of each, but who has time to measure).  Add about 1 and 1/2 cups of \nrice (measure by sight, accuracy is not important) and stir, "toast" the rice, \nuntil the rice absorbs the liquid that the veggies have given off.  Add about a \ncup of wine and let rice absorb the liquid.  Generously cover the rice with \nwarm water (or mushroom stock or veggie stock).  Stir, scrape the bottom to \nensure there is no sticking.  Gently keep adding liquid as the rice absorbs \nliquid in pot, adding salt and pepper to taste.  Careful not to over stir, just \nkeep the rice from sticking as it drinks!  When the rice is nearly prefect (a \nlittle white in middle of grain) introduce the mushrooms.  Add in parsley and \nperhaps more lemon and wine, if you like, at the last minute. Viola!  Serve \ntopped with shredded or slivered cheese, if you wish.  As you enjoy this \nrisotto, think about other variations that are well within your masterchef \nrepertoire: for instance, adding some peas and prosciutto could be lovely, or \neven cream and sage instead of lemon and parsley.</p>', 'title': 'Mushroom Risotto', 'ingredients': '1 onion; 2 garlic cloves; 2 lemons; parsley; 1 cup white wine; olive oil', 'prep': '10 min', 'cook': '40 min'}
----------------
{'slug': '2018-06-25-pancakes', 'build_path': './build/articles/Cooking/2018-06-25-pancakes.html', 'category': 'Cooking', 'url': '/articles/Cooking/2018-06-25-pancakes.html', 'date': datetime.datetime(2018, 6, 25, 0, 0), 'content_html': "<h1>August 18, 2018</h1>\n<p>I made a single batch, yielded about a dozen pancakes, probably should have done \na double so there was enough to freeze</p>\n<h1>June 24, 2018</h1>\n<p>They look satisfyingly thick and fluffy. The almond flour contains a lot more \nprotein and gives these more substance than other pancake recipes I've tried. </p>\n<h1>October 20, 2018</h1>\n<p>Added extra milk by accident (1/2 cup total vs 1/4 normal) they seemed fluffier \nthan normal.</p>\n<h1>Feb 17, 2019</h1>\n<p>Replaced milk with buttermilk because we happened to have it. They came out\nfluffier and richer. I may have added a bit more than 1/2 cup. There usually is \na slightly bitter flavor from the flour in the prior batches, but that was gone \nwith buttermilk. I also added a tablespoon of buckwheat because we had it. \nCouldn't taste it.</p>\n<pre><code>2/3 cup whole wheat flour\n2/3 cup Almond flour (Bob's red mill)\n1/2 tsp baking soda\n1/4 tsp sea salt\nMIX\n3 large pastured eggs, room temperature\n1/2 cup buttermilk (or whole milk if you don't have it)\n1 tbsp unsalted butter (or coconut oil), melted\n1 tbsp honey (or maple syrup)\n1 tsp pure vanilla extract\n1/4 tsp apple cider vinegar\n</code></pre>", 'title': 'Pancakes (Almond flour)', 'ingredients': ';;;', 'prep': '20 min', 'cook': '30 min'}
----------------
{'slug': '2018-07-06-quiche', 'build_path': './build/articles/Cooking/2018-07-06-quiche.html', 'category': 'Cooking', 'url': '/articles/Cooking/2018-07-06-quiche.html', 'date': datetime.datetime(2018, 7, 6, 0, 0), 'content_html': "<p>Mom's favorite recipe, she likes to add scallops and broccoli (cooked ahead of \ntime). It has a light and fluffy taste because there is no cheese. Double the \nrecipe if making it in a standard pie plate.</p>\n<ol>\n<li>Roll out dough and fit to pie plate</li>\n<li>Set rack in lowest position in oven and preheat to 375 degrees</li>\n<li>Mix eggs in mixing bowl. When done, add cream and milk, and spices. Mix a bit\nmore</li>\n<li>Pour filling into pie plate. Add additional pre-cooked ingredients as well, \nsuch as scallops, broccoli, etc. </li>\n<li>Bake for 40 minutes. You will know it is done when a cake tester (or knife) \ncome out cleanly from the senter. </li>\n<li>Cool for 20 minutes before serving.</li>\n</ol>\n<p>Pro-tips: serve with a salad as a lunch/ brunch/ dinner meal. Serve with fruit \nsalad for a breakfast/ brunch meal. Alternate fillings include cheese (Gruyere, \nEmmenthal, swiss, cheddar).</p>", 'title': "Quiche (Mom's recipe)", 'ingredients': '1/2 cup milk; 1/2 cup heavy cream; 3 large eggs; 1/4 teaspoon salt; 1/4 teaspoon ground pepper; 1/4 teaspoon ground nutmeg;', 'prep': '20 min', 'cook': '30 min'}
----------------
{'slug': '2020-09-14-empanadas', 'build_path': './build/articles/Cooking/2020-09-14-empanadas.html', 'category': 'Cooking', 'url': '/articles/Cooking/2020-09-14-empanadas.html', 'date': datetime.datetime(2020, 9, 14, 0, 0), 'content_html': '<h3>Beef</h3>\n<p>1 lbs beef\n1 tablespoon onion powder\n1 teaspoon papryka\nonions chopped\ngarlic\npeas\na little bit of broth\na little bit of flour to thicken it before going into the pie crust</p>\n<h3>Steps</h3>', 'title': 'Empanadas', 'ingredients': None, 'prep': '20 min', 'cook': '25 min'}
----------------
{'slug': '2016-07-19-refridgerator-muffins', 'build_path': './build/articles/Cooking/2016-07-19-refridgerator-muffins.html', 'category': 'Cooking', 'url': '/articles/Cooking/2016-07-19-refridgerator-muffins.html', 'date': datetime.datetime(2016, 7, 19, 0, 0), 'content_html': '<p>Recipe came from my Mom. We used to eat these growing up. She would make a batch \non the weekend and store the batter in the refrigerator. Then, each morning, we\nwould get fresh muffins baked. Cook time is 30 min when putting the batter in a \ncold oven and heating it up to 400 degrees.</p>\n<h3>Steps</h3>\n<ol>\n<li>Place above in separate bowl and add ½ cup of boiling water. Let sit while \nyou combine the rest of the ingredients.</li>\n<li>Add sugar and oil to mixer and mix well, frothing.</li>\n<li>Add eggs, one at a time.</li>\n<li>Sift dry ingredients and add alternately with buttermilk. </li>\n<li>Add the cereals, mixing well. End with a stir by hand.</li>\n<li>Refrigerate and bake off as needed.</li>\n<li>Bake at 400 for 20-30 minutes</li>\n</ol>', 'title': 'Refrigerator Muffins', 'ingredients': "2 cups Kellogg's original All-Bran cereal; 4 large shredded wheat, crushed (POST original shredded wheat big biscuit); 1 cup canola oil; 2 ½ cups sugar; 1 quart buttermilk; 4 eggs; 5 cups all purpose flour; 5 tsp baking soda; ½ tsp baking powder; 2 tsp salt", 'prep': '20 min', 'cook': '30 min (400 degrees)'}
----------------
{'slug': '2016-07-13-grandma-s-banana-bread', 'build_path': './build/articles/Cooking/2016-07-13-grandma-s-banana-bread.html', 'category': 'Cooking', 'url': '/articles/Cooking/2016-07-13-grandma-s-banana-bread.html', 'date': datetime.datetime(2016, 7, 13, 0, 0), 'content_html': "<p>This banana bread is more of a cake due to the flour to banana ratio. </p>\n<h3>Steps</h3>\n<ol>\n<li>Preheat - put oven rack in middle and preheat oven to 350 °F. Grease your pan and powder with four, or use wax paper.</li>\n<li>Curdle Milk - squeeze 1 Tbs lemon juice into 2/3 cup milk and let it sit until the milk curdles. About 1 minute.</li>\n<li>Mix Dry Ingredients - in a large bowl, mix: 2 ½ cup sifted cake flour, ½ tbs baking soda, ½ tbs baking powder and ¾ salt.</li>\n<li>Beat Together - In your blender beat: 1/2 cup butter and 2/3 cup sugar. Once they are smooth, add in 2 eggs, 1 at a time. Finally, add in the curdled milk.</li>\n<li>Mix In - Add to your blender: 3 bananas, slowly fold in your Dry Ingredients. Pour the ingredients into your prepared pan.</li>\n<li>Bake - Bake until a toothpick comes away clean. Let sit ~15 minutes before trying to remove from the pan.</li>\n</ol>\n<h3>Notes</h3>\n<p>I sprinkle nuts on the top of the loaf after I've poured the batter instead of \nmixing them randomly into the batter. This keeps the texture of each bite \ncontsistet. I also toast the nuts before I add them for extra flavor.</p>", 'title': 'Banana Bread Cake', 'ingredients': '2/3 cup whole milk; 1 Tbs fresh lemon juice; 2 1/2 cup sifted cake flour; 1 tbs baking powder; 3/4 tbs salt; 1/2 tbs baking soda; 1/2 cup butter; 2/3 cup sugar; 2 large eggs; 3 ripe bananas', 'prep': '20 min', 'cook': '1 hour'}
----------------
{'slug': '2016-06-06-no-kneed-bread', 'build_path': './build/articles/Cooking/2016-06-06-no-kneed-bread.html', 'category': 'Cooking', 'url': '/articles/Cooking/2016-06-06-no-kneed-bread.html', 'date': datetime.datetime(2016, 6, 6, 0, 0), 'content_html': '<p>Combine flour, yeast and salt in a large bowl. Add 1 1/2 cups water and stir \nuntil blended; dough will be shaggy. Cover bowl with plastic wrap. Let dough \nrest about 4 hours at warm room temperature, about 70 degrees.</p>\n<p>Lightly oil a work surface and place dough on it; fold it over on itself once or\n twice. Cover loosely with plastic wrap and let rest 30 minutes more.</p>\n<p>At least a half-hour before dough is ready, heat oven to 450 degrees. Put a \n6-to-8-quart heavy covered pot (cast iron, enamel, Pyrex or ceramic) in oven as \nit heats. When dough is ready, carefully remove pot from oven. Slide your hand \nunder dough and put it into pot, seam side up. Shake pan once or twice if dough \nis unevenly distributed; it will straighten out as it bakes.</p>\n<p>Cover with lid and bake 30 minutes, then remove lid and bake another 15 to 30 \nminutes, until loaf is beautifully browned. Cool on a rack.</p>', 'title': 'NYTimes No Kneed Bread', 'ingredients': '3 cups bread flour; 1 packet (1/4 ounce) instant yeast; 1 1/2 teaspoons salt; Oil as needed'}
----------------
{'slug': '2020-10-25-Beef-Stew', 'build_path': './build/articles/Cooking/2020-10-25-Beef-Stew.html', 'category': 'Cooking', 'url': '/articles/Cooking/2020-10-25-Beef-Stew.html', 'date': datetime.datetime(2020, 10, 25, 0, 0), 'content_html': '<h3>Steps</h3>\n<p>Melt the bacon fat or butter in the bottom of a large Dutch oven over medium-high heat and add the onion, celery, carrots, and garlic. Season the vegetables well with salt and pepper. Cook until the vegetables have softened, about 10 minutes. Add the tomato paste and stir to coat the vegetables. Cook until the tomato paste darkens and sticks a bit to the bottom of the pot, about 2 more minutes.</p>\n<p>Add the chicken stock and lentils and bring to a boil. Reduce the heat to maintain a very gentle simmer and add the chicken thighs. Cover and continue cooking for 1 hour, checking periodically to make sure the soup isn’t bubbling too vigorously. Remove the chicken thighs and put them on a plate to cool.</p>\n<p>Working in batches, blend about half of the lentil soup until it’s thick and creamy. (You can blend all of it if you like a creamy lentil soup, or none of it if you want it completely brothy. I like it somewhere in the middle.) Stir the pureed soup back into the pot. Using two forks, shred the chicken and return it to the pot. Taste and correct the seasoning.</p>', 'title': 'Beef Stew', 'ingredients': '4 tablespoons bacon fat or butter; 1 large onion, diced; 2 stalks celery, diced; 2 carrots, diced; 2 garlic cloves, thinly sliced; 2 tablespoons tomato paste; 12 cups chicken stock or water; 2 cups French lentils; 1 pound boneless, skinless chicken thighs (See Recipe Note); Grated Parmesan or Romano cheese, for serving'}
----------------
{'slug': '2019-01-05-new-england-baked-beans', 'build_path': './build/articles/Cooking/2019-01-05-new-england-baked-beans.html', 'category': 'Cooking', 'url': '/articles/Cooking/2019-01-05-new-england-baked-beans.html', 'date': datetime.datetime(2019, 1, 5, 0, 0), 'content_html': "<h3>Steps</h3>\n<ol>\n<li>Soak, rinse, wash navy beans in water until it runs clean. </li>\n<li>Soak navy beans overnight in water (10-12 hrs)</li>\n<li>Put all ingredients in pot</li>\n<li>Instant pot on 'Beans' setting for 55 minutes</li>\n<li>Instant pot 'saute' setting to boil off excess water until the consistency you are looking for</li>\n</ol>\n<h3>Notes</h3>\n<p>4/21/2019\nput them in the instant pot for 65 minutes this time, since the 55 minutes was just shy last time. They look a little over cooked :( Here is the ingredients from the original recipe: </p>\n<p>1 lbs dried navy beans (2.5 cups); 1/4 lbs think bacon; 1 medium onion; 1/4 cup maple syrup; 1/3 cup molasses; 1/4 cup brown sugar (packed); 1 tablespoon worchestershire sauce; 1 table spoon ground mustard seed; 1/2 teaspoon salt; fresh ground black pepper; 1/4 teaspoon apple cider vinager</p>\n<p>3/21/2019\nI started buying navy beans in bulk because Whole Foods stopped selling the 1lbs bag. I used 2 cups instead of 1lbs. I didn't have maple suryp, so I ignored it. Beans turned out great. Still needed to be reduced in instant pot afterwards. </p>\n<p>2/21/2019\nI changed the recipe from 1/4 cup to 1/4 teaspoon apple cider vinegar. I think that was a typo. I also used 1/3 cup brown sugar for a double batch. Also, only did 1 onion. </p>\n<p>Produces 8 cups (fills 2 of our circular anchor food containers). I scooped out the bacon and onions out from the top of the watery mixture. The beans were a little under cooked at 55 min. I had to reduce the mixture for another 40 minutes on sauté setting to thicken the sauce. This also cooked the beans perfectly. </p>\n<p>It was a bit too sweet. Maybe cut the brown sugar in half next time. </p>\n<p>6/1/2020\nCut the brown sugar in half this time, or 2/3's of the 1/4 cup filled. We cooked for 59 minutes. Making during the pandemic. It has been difficult to find navy beans. </p>\n<p>8/9/2020\nSame as the 6/1 recipe. Still in COVID-19 world. It is now possible to find navy beans, but still much harder than it used to be.</p>", 'title': 'New England Baked Beans', 'ingredients': '1 lbs dried navy beans (2.5 cups); 1/4 lbs think bacon; 1 medium onion; 1/4 cup maple syrup; 1/3 cup molasses; 1/8 cup brown sugar (packed); 1 tablespoon worchestershire sauce; 1 table spoon ground mustard seed; 1/2 teaspoon salt; fresh ground black pepper; 1/4 teaspoon apple cider vinager', 'prep': '20 min', 'cook': '1 hour'}
----------------
{'slug': '2016-07-19-chicken-lentil-soup', 'build_path': './build/articles/Cooking/2016-07-19-chicken-lentil-soup.html', 'category': 'Cooking', 'url': '/articles/Cooking/2016-07-19-chicken-lentil-soup.html', 'date': datetime.datetime(2016, 7, 19, 0, 0), 'content_html': '<h3>Steps</h3>\n<p>Melt the bacon fat or butter in the bottom of a large Dutch oven over medium-high heat and add the onion, celery, carrots, and garlic. Season the vegetables well with salt and pepper. Cook until the vegetables have softened, about 10 minutes. Add the tomato paste and stir to coat the vegetables. Cook until the tomato paste darkens and sticks a bit to the bottom of the pot, about 2 more minutes.</p>\n<p>Add the chicken stock and lentils and bring to a boil. Reduce the heat to maintain a very gentle simmer and add the chicken thighs. Cover and continue cooking for 1 hour, checking periodically to make sure the soup isn’t bubbling too vigorously. Remove the chicken thighs and put them on a plate to cool.</p>\n<p>Working in batches, blend about half of the lentil soup until it’s thick and creamy. (You can blend all of it if you like a creamy lentil soup, or none of it if you want it completely brothy. I like it somewhere in the middle.) Stir the pureed soup back into the pot. Using two forks, shred the chicken and return it to the pot. Taste and correct the seasoning.</p>', 'title': 'Chicken Lentil Soup', 'ingredients': '4 tablespoons bacon fat or butter; 1 large onion, diced; 2 stalks celery, diced; 2 carrots, diced; 2 garlic cloves, thinly sliced; 2 tablespoons tomato paste; 12 cups chicken stock or water; 2 cups French lentils; 1 pound boneless, skinless chicken thighs (See Recipe Note); Grated Parmesan or Romano cheese, for serving'}
----------------
{'slug': '2019-08-09-lentil-sausage-soup', 'build_path': './build/articles/Cooking/2019-08-09-lentil-sausage-soup.html', 'category': 'Cooking', 'url': '/articles/Cooking/2019-08-09-lentil-sausage-soup.html', 'date': datetime.datetime(2019, 8, 9, 0, 0), 'content_html': '<h3>Steps</h3>', 'title': 'Lentil Soup', 'ingredients': '1 cup French lentils; 3 cloves garlic; 1 medium onion; 1 cup chopped carrots; 1/2 cup celery; 1 box broth (chicken or beef); 2 sausages (optional); 1 cup graded paramaggiano cheese;', 'prep': '20 min', 'cook': '25 min'}
----------------
{'slug': '2020-10-02-blueberry-oatmeal-muffin', 'build_path': './build/articles/Cooking/2020-10-02-blueberry-oatmeal-muffin.html', 'category': 'Cooking', 'url': '/articles/Cooking/2020-10-02-blueberry-oatmeal-muffin.html', 'date': datetime.datetime(2020, 10, 2, 0, 0), 'content_html': '<p>Internet </p>\n<h3>Steps</h3>\n<pre><code>Combine milk and oats. Set aside for 20 minutes so the oats puff up and soak up some moisture. This is crucial to the recipe! (I usually melt the butter now so it has a few minutes to cool.) Don’t do this the night before as that’s too long for soaking. If you find the oats haven’t soaked up any moisture after 20 minutes, give it a stir and wait 10 more minutes.\nPreheat oven to 425°F (218°C). Spray a 12-count muffin pan with nonstick spray or use cupcake liners.\nWhisk the flour, baking powder, baking soda, cinnamon, and salt together in a large bowl until combined. Set aside. Whisk the melted butter, honey, egg, and vanilla extract together in a medium bowl until combined. Pour the wet ingredients into the dry ingredients, stir a few times, then add the soaked oats (milk included, do not drain) and blueberries. Fold everything together gently just until combined.\nSpoon the batter into liners, filling them all the way to the top. Top with oats and a light sprinkle of coconut sugar, if desired. Bake for 5 minutes at 425 then, keeping the muffins in the oven, reduce the oven temperature to 350°F (177°C). Bake for an additional 16-17 minutes or until a toothpick inserted in the center comes out clean. The total time these muffins take in the oven is about 22-23 minutes, give or take. (For mini muffins, bake 11-13 minutes at 350°F (177°C).) Allow the muffins to cool for 5 minutes in the muffin pan, then transfer to a wire rack to continue cooling.\nMuffins stay fresh covered at room temperature for a few days, then transfer to the fridge for up to 1 week.\n</code></pre>\n<p>Notes</p>\n<pre><code>Make Ahead Instructions: For longer storage, freeze muffins for up to 3 months. Allow to thaw overnight in the refrigerator, then bring to room temperature or warm up in the microwave if desired.\nMilk: I tested the blueberry muffins with unsweetened almond milk, but any milk, dairy or nondairy, can work. If using frozen berries, see note below.\nOats: I recommend whole oats. Avoid steel cut oats, quick oats, and instant oats. Steel cut oats won’t absorb the milk and quick/instant are too thin and will dissolve in the batter as it cooks.\nFlour: For best taste and texture, I recommend all-purpose flour. If you try whole wheat flour, let me know how they turn out!\nOil: You can use canola, vegetable, or melted coconut oil instead of the butter, but the flavor will change. I strongly recommend melted butter.\nSugar: You can use 1/2 cup maple syrup, coconut sugar, or packed light or dark brown sugar instead of honey.\nBerries: If using frozen berries, do not thaw and reduce the milk down to 3/4 cup (180ml). Frozen berries give off so much moisture and the muffins taste a little too wet. Reducing the milk helps. No other changes to the recipe ingredients or instructions.\nWhy the Initial High Temperature? The hot burst of air will spring up the top of the muffin quickly, then the inside of the muffin can bake for the remainder of the time. This helps the muffins rise nice and tall.\nNutrition: Using SparkRecipes calculator and calculated using unsweetened almond milk these muffins come out to 205 calories, 9g fat, 28g carbs, 2g fiber, and 4g protein each.\n</code></pre>', 'title': 'Blueberry Oatmeal Muffin', 'ingredients': '1 cup (240ml) milk; 1 cup (80g) thick rolled oats; 1.25 cups (156g) whole wheat flour; 1 tsp baking powder; 1/2 tsp baking soda; 1/2 tsp cinnamon; 1/2 tsp salt; 1/2 cup unsalted butter, melted; 1/2 cup maple syrup; 1 large egg, room temp; 1 tsp pure vanilla extract; 1 cup frozen blueberries', 'prep': '20 min', 'cook': '30 min (450 degrees)'}
----------------
{'slug': '2016-07-13-chocolate-coffee-bourbon-cake', 'build_path': './build/articles/Cooking/2016-07-13-chocolate-coffee-bourbon-cake.html', 'category': 'Cooking', 'url': '/articles/Cooking/2016-07-13-chocolate-coffee-bourbon-cake.html', 'date': datetime.datetime(2016, 7, 13, 0, 0), 'content_html': "<p>This cake combines the best flavors of coffee, bourbon and chocolate in a very warm and comforting way (you don’t have a problem if you find bourbon comforting, right?). Very flavorful, perfect for a Fall and Winter cake.</p>\n<p>Like most cakes, this one tastes best if you give it a day or so to sit and think about what you've done before you serve it. Make sure to keep it covered to stay moist. Great if served warm, or room temperature with coffee for breakfast.</p>\n<h3>Steps</h3>\n<p>Pre-heat oven to 325\nCoat the inside of your cake pan with butter, dust with cocoa powder\nMedium Bowl – sift:\n2 cups flower\n1 ¼ teaspoon baking soda\n½ teaspoon salt\nSauce pan – over low heat:\nMelt 1 cup butter\nAdd 1 ½ cup coffee\nAdd ½ cup bourbon\nAdd in Cocoa a little at a time while continuing to stir\nTake off the heat and stir in 2 cups white sugar\nLet sit and cool for about 5 minutes\nIn a large mixing bowl – like your kitchenaid bowl…\nAdd contents from the sauce pan\nAdd 2 eggs\nAdd 1 teaspoon vanilla\nAdd 1 teaspoon almond extract\nMix with the paddle on a slow setting\nSlowly add in contents of the medium bowl (flour, etc) while continuing to mix\nNote: mixture will still be a little on the watery side at the end, that’s okay!\nPour batter into cake pan. If Bundt cake pan, maybe fill 75-80%, as it will rise. (Extra batter makes good cupcakes)\nCook about 50 minutes in middle rack (until toothpick comes out clean)</p>\n<h3>Frosting Options</h3>\n<p>There’s a lot of options that go well with this cake, so you’re going to have to make some difficult choices.\nIce Cream\nConfectioner’s Sugar – tip, use a flour sifter to sprinkle the sugar on top of the cake.\nWhipped cream – add a little bit of sugar. If you’re feeling spunky than you can fold in some molasses, or add a few things to spice it up, like coffee, bourbon, vanilla or almond extract.</p>", 'title': 'Chocolate Coffee Bourbon Cake', 'ingredients': '1 cup cocoa powder (unsweetened, not Dutch-processed); 1 ½ cups brewed coffee (get a 20 oz drip from your best local coffee shop, will leave just enough for you to drink while baking); 1/2 cup bourbon (Makers); 1 cup unsalted butter; 2 cups sugar; 2 cups flour; 1 1/4 tsp baking soda; 1/2 tsp salt; 2 large eggs; 1 tsp vanilla', 'prep': '30 min', 'cook': '1 hour'}
----------------
{'slug': '2013-03-07-basic-cms-in-ruby', 'build_path': './build/articles/Development/2013-03-07-basic-cms-in-ruby.html', 'category': 'Development', 'url': '/articles/Development/2013-03-07-basic-cms-in-ruby.html', 'date': datetime.datetime(2013, 3, 7, 0, 0), 'content_html': '<p>For my hobby project this winter, I wanted to try out a new language, something that was distinctly non-microsoft. I decided to try out Ruby to learn about the language before trying out a bigger project in Rails. I have been a fan of ASP.net MVC, so it was exciting to learn more about where these designs came from. I created a simple blog framework on top of Rack.</p>\n<ol>\n<li><a href="https://github.com/nbuggia/baron-blog">Blog Template on GitHub</a></li>\n<li><a href="https://github.com/nbuggia/baron-blog-engine-gem">Blog Framework Gem on GitHub</a></li>\n</ol>\n<p>So far I like Ruby a lot. It has all the benefits of an inturpretted language. It also has a large and active community that has built everything you can imagine, open sources and complete with unit tests and documentation. </p>\n<p>The packaging system is called <a href="https://rubygems.org">GEMs</a>, also written in Ruby. Works really well. I used a gem (called <a href="https://rubygems.org/gems/jeweler">Jeweler</a>) to encapsulate all the logic of the blog engine and publish the <a href="https://rubygems.org/gems/baron">blog engine</a> to the packaging system library. The Blog Template project includes the blog template and content. It refrences the blog engine gem.</p>\n<p>One of the principles of the Ruby community is that languages should be easy to read and expressive. To support that, they use the concept of domain specific languages (DSL) to create special purpose, expressive languages for everything. My favorite was <a href="https://cucumber.io/">Cucumber</a>, a langauge for expressing use cases and aligning them with automated tests. Another application, RSpec, is used for testing. </p>\n<p>The Ruby language has some functionality that make it very productive. In the example method below, <code>get_all_categories</code>, the <code>map</code> method takes the list of folders inside your blog, builds a poor-man\'s struct (e.g. JSON) to turn each one into a category object, and then sorts them by their name property. It may not be the fastest, but the productivity gains are a worthwhile tradeoff in many cases. </p>\n<pre><code>def get_all_categories\n    Dir["#{get_articles_path}/*/"].map do |a| \n    folder_name = File.basename(a)\n    {\n        name: titlecase(folder_name),\n        node_name: folder_name.gsub(\' \', \'-\'),\n        path: "/#{@config[:permalink_prefix]}/#{folder_name.gsub(\' \', \'-\')}/".squeeze(\'/\'),\n        count: Dir["#{get_articles_path}/#{folder_name}/*"].count \n    }\n    end .\n    sort_by { |hash| hash[:name] }\nend\n</code></pre>\n<p>The community is very active. There were a lot of great resources to get me started. A few unique to the ruby community include: </p>\n<ul>\n<li>Railscasts: <a href="http://railscasts.com/">http://railscasts.com/</a></li>\n<li>Rails for zombies: <a href="http://railsforzombies.org/">http://railsforzombies.org/</a> </li>\n<li>Rspec: <a href="http://rspec.info/">http://rspec.info/</a></li>\n</ul>\n<p>This was a particularly memorable project because I wrote it while on a brief sabbatical I took in Barcelona to complete a Spanish immersion program. For a month over Christmas 2013, I stayed with the welcoming and generous Fernandez family just outside of Eixample, where the Sagrada Familia is almost complete. I would have breakfast with the family, go to classes until 1pm, and then I would go to a library for a couple hours to code, before walking around the city until midnight. A couple of my favorite places were <a href="https://www.google.com/search?q=La+Foixarda">La Foixarda</a>, the <a href="https://www.google.com/search?q=gothic+quarter+barcelona">Gothic Quarter</a> and <a href="https://en.wikipedia.org/wiki/Montju%C3%AFc">Montjuic</a>.</p>', 'title': 'Basic Blog Framework in Ruby'}
----------------
{'slug': '2011-10-23-browser-view-controller-iphone', 'build_path': './build/articles/Development/2011-10-23-browser-view-controller-iphone.html', 'category': 'Development', 'url': '/articles/Development/2011-10-23-browser-view-controller-iphone.html', 'date': datetime.datetime(2011, 10, 23, 0, 0), 'content_html': '<p>Github source: <a href="https://github.com/nbuggia/Browser-View-Controller--iPhone-">Browser View Controller</a></p>\n<p>iPhone apps often have the need to show a web page, and the easiest way to implement this is to have the page opened in Safari. The problem with this, is that now your customer is stuck in Safari, and they might not know how to get back into your app. This project gives you all the boilerplate code you need to create a smooth experience opening web pages within your app, and seamlessly get back with one click.</p>\n<p><img alt="screen shot 1" src="/images/articles/Browser-View-Controller_thumb1.png">\n<img style="padding-left:2em;" alt="screen shot 2" src="/images/articles/Browser-View-Controller-1_thumb.png"></p>\n<p>Here are the scenarios implemented:</p>\n<ul>\n<li><strong>Opening a URL</strong> from within a method – useful for opening links triggered by a UIButton or UITableView.</li>\n<li><strong>Opening a URL from within a UITextView</strong> – useful for links embedded within text strings that UITextView can automatically identify and turn into clickable hyperlinks.</li>\n<li><strong>Opening a URL from within a UIWebView</strong> – useful for when you are using a UIWebView to render formatted text in your application with hyperlinks.</li>\n</ul>\n<h2>Getting Started</h2>\n<p>Please see GitHub for instructions on using the library:\n<a href="https://github.com/nbuggia/Browser-View-Controller--iPhone-">https://github.com/nbuggia/Browser-View-Controller--iPhone-</a></p>\n<p>Please let me know if there are any additional features you would like to see in the comment section below!</p>\n<h2>Thanks</h2>\n<p>I’d like to thank <a href="http://penandthink.com/">Joseph Wain</a> of <a href="http://glyphish.com/">Glypish</a> fame for providing the arrow icons, and making them freely available to everyone. Go buy the <a href="http://glyphish.com/">best iphone icons</a> from Glypish!</p>\n<p>I’d also like to thank <a href="http://www.qrayon.com/">Chen-I Lim</a> for the fix for getting this to work with the Facebook auth system. Go check out his many wonderful apps from Qrayon.</p>\n<h2>Other options</h2>\n<p>There is one well-known library that does this today: <a href="https://github.com/facebook/three20/blob/master/src/Three20UI/Sources/TTWebController.m">TTWebController</a>, which is part of the well known <a href="https://github.com/facebook/three20">Three20</a> iOS library published by Facebook as open source. The only problem with this library is that it requires you to incorporate the whole of Three20 in your app, and doesn’t help with opening links in UIWebViews or UITextViews. However, it is still a solid, well written library you should consider.</p>\n<p>References</p>\n<p><a href="http://stackoverflow.com/questions/2543967/how-to-intercept-click-on-link-in-uitextview">How to Intercept Clicks on Links in UITextView</a> – stackoverflow.com</p>', 'title': 'Browser View Controller (iPhone)'}
----------------
{'slug': '2013-09-13-hiring', 'build_path': './build/articles/Management/2013-09-13-hiring.html', 'category': 'Management', 'url': '/articles/Management/2013-09-13-hiring.html', 'date': datetime.datetime(2013, 9, 13, 0, 0), 'content_html': '<p>In all my experiences as a manager, none are more stressful and difficult than hiring. Probably because none are more critical to your professional success and day-to-day happiness. Working with great people is one of the things that makes life exceptional, but finding and hiring them can be a tremendous challenge - especially now in the technology field. </p>\n<p>During my first year as a manager, I became keenly aware of the differences between my team and other managers\' teams. Mine was full of open-positions and high aspirations, while other managers had filled their teams with actual rockstars and were reporting on actual results. I couldn\'t figure out how they were able to do their day jobs, do the work of their open-positions <em>and</em> still have time to hire great people.</p>\n<p>Flash ahead six years through a lot of trial &amp; error and outright stealing of techniques, I\'ve mostly figured out how to be one of the managers I aspired to be. And now I find myself mentoring new managers who are going through the same trails. I\'ve tried to simplify what I\'ve learned into this article so they will (hopefully) not have to go through all the same pain. </p>\n<h2>What Are You Really Looking For?</h2>\n<p>It is important to spend time to really understand the need that you want to fulfill with this position. If you are backfilling a vacated position, you should recosider the role incase things have changed. </p>\n<p><strong>Do you appreciate diversity?</strong></p>\n<p>The first tendency of most new managers (myself included) is to hire people who are exactly like themselves. This leads to a very myopic understanding of people being either \'good\' (like me) or \'bad\' (not like me). More seasoned managers have developed a nuanced understanding of, and appreciation for the different types of work styles and skills. They use this to expand their world view and find new ways to tackle new challenges. Great employees are also looking for this. </p>\n<p>This is especially important in agile organizations that change quickly. Positions tend to change significantly every 6 months - 2 years, so having a balanced pool of talent makes it much easier to respond to whatever the next challenge will be. You should also think of balancing talent across your manager\'s org as well.</p>\n<h2>Creating A Great Place to Work</h2>\n<p>The best place to improving your hiring is to make sure that your team is an amazing place to work. The happier and more successful your team is, the less likely they will be to leave, and everyone who interacts with your team will start to feel a gravitational pull. Here\'s what great employees look for in a workplace: </p>\n<p><strong>Does your team do interesting and meaningful work?</strong></p>\n<p>It doesn\'t matter which team you manage: <em>Future Product Strategy,</em> or <em>Legacy App Operations</em>, any team can be made an interesting or uninteresting place to work. <em>Interesting</em> is all about the types of challenges team members will face, and the culture the Manager provides. Are the challenges new, or is it reinventing the wheel? Are the challenges achievable, or completely unrealistic? Does the culture reward well-thought out risk taking and pushing boundaries? Do you strive to automate the crappy work to make more time for high impact work? </p>\n<p>Great employees want an opportunity to think creatively, an environment in which they can make progress and have a real impact. But when Management creates a team, they only think about the Work they need staffed. It is <em>always</em> up to the manager to figure out how to frame the Work in a meaningful way, and to deliver on it in an interesting way. The good news is that management will appreciate these things once the Manager has structured them this way. Everyone wants to be part of something interesting and meaningful, even management.</p>\n<p><strong>Is your team setup for success?</strong></p>\n<p>Very few new teams are setup for success by default. This means that the Manager has clarified: the objectives, stakeholders/ dependencies, priorities, timeline, the way to measure progress, the way to report status, and the decision making process. The Manager also needs to ensure all this is aligned to the organization charter, has a good relationships, tackles the tough problems early, is a great communicator, and doesn\'t sweat change. That\'s a lot of work, but that is why you get paid the big bucks.</p>\n<p><strong>Will you provide a career?</strong></p>\n<p>Great people require more than just a job, you need to show them how a component of this role will help them grow as professionals, and that you are committed to supporting them in that process. Great people are also willing to do the crappy work every team has, as long as the burden is shared, and the Manager is working to minimize it. The best proof you can have, is a stable of employees that all have good jobs, professional growth to talk about, and a track record of people being promoted to bigger and better things.</p>\n<h2>Developing Your Personal Network</h2>\n<p>You\'ve posted your job to Linked In, Monster, your company\'s website, and now you\'re getting thousands of applications. Awesome! Unfortunately, 90% of candidates are applying aspirationally, and you are burning hours to sort through and find the qualified ones, only to learn that by the time you\'ve found them, they are deep into interviews with your competition. D\'oh! </p>\n<p>This is why your personal network is still the most valuable asset in finding great talent. It can take a long time to build a valuable network, but the more energy you devote, the more valuable it will become. Over the past couple years, my personal network has accounted for about 50% of all of my hiring. And people I hire from my network tend to start faster and have more predictable performance.</p>\n<p>Here are a few tips for developing your network:</p>\n<ul>\n<li>Every time you loose a good candidate in a loop, find a way to stay in touch with them. Sooner or later they will be looking for their next opportunity and then you get another shot to see if there is something you know of that will be a great fit.</li>\n<li>Actively work on side projects with other interesting people. This gives you an opportunity to meet people outside of your normal circles and get first hand accounting of their particular talents.</li>\n<li>Find opportunities to write and speak, it is a great way to get your name out there and you may find that great candidates might start coming directly to you. And, when a great candidate does come your way, this content will make you much more interesting to them.</li>\n<li>Actively invest in your network, whether it is just making connections, mentoring, helping other people hire, sharing a drink or taking them rock climbing. This deepens you relationships with these folks and gives you something to talk with them about.</li>\n<li>Treat everyone you meet with respect and compassion, no matter if they are a lowly intern or a completely unqualified, aspirational candidate. It is a small world and we live a long time. You never know when that person will be in a position to give you a job, or will come up with the next big idea. Then they will be much more likely to treat you in kind.</li>\n</ul>\n<h2>Interviewing</h2>\n<p>Interviewing is difficult and you are going to make a lot of mistakes, especially in the beginning. But don\'t feel too badly about it, the whole tech industry is still learning a lot about the interview process. Google recently conducted some <a href="http://www.nytimes.com/2013/06/20/business/in-head-hunting-big-data-may-not-be-such-a-big-deal.html">internal research</a> and found out that many of our industry conventions are just plain wrong. For example, they found that <a href="http://www.mytechinterviews.com/category/puzzles">brainteasers</a>, GPAs and standardized test scores don\'t correlate with future employee success.</p>\n<p><strong>Job Descriptions</strong></p>\n<p>Within a company, most job descriptions look the same, especially for the high volume roles like those with ~5 years experience. Make your job descriptions stand out. The key is to get past the generic parts so you can talk about how interesting and meaningful the work is, and how great of a place the team is to work. Don\'t forget to include some of the challenges the employee will face, it makes the position real and people like to overcome challenges.</p>\n<p>The one exception to this is the secret project that you can\'t actually talk about. For this type of project, the traditional job description probably isn\'t the right approach. You don\'t want to be the manager who caused the PR incident and gave away the secret to the compeition.  </p>\n<p><strong>Qualify Candidates</strong></p>\n<p>When reviewing candidates from the high volume sources like Linked In or your company\'s website, you will need a process to narrow down the list to just the few qualified candidates that you want to interview. I recommend you get help with this activity by distributing some of the work to your team, or peer teams if you don\'t have any employees yet. This is a great opportunity to get them involved in the hiring and to have a broader discussion about what great candidates should look like. Use standardized phone screen, and track results in shared spreadsheets. </p>\n<p>I highly recommend asking people for a portfolio with a few samples of their best work, this can really help you see what they are likely to produce on your team.</p>\n<p>After the initial screen Managers should always do an informational interview before setting up a loop to ensure it is worth your team\'s time.</p>\n<p><strong>The Loop</strong></p>\n<p>As with many things, the more you invest into an interview loop, the more you are likely to get out of it. I highly recommend that you email interviewers in advance with a summary of the candidate, the position and what you expect from each of them. You should also include all major stakeholders for role on the loop so you can check for a cultural fit in addition to a skills fit.</p>\n<p>You should instruct interviewers to only share hire/ no hire decisions and feedback with you. Otherwise, this creates peer pressure on the rest of the interviewers and can affect their decisions - especially strong recommendations from senior team members.</p>\n<p><strong>The Interview</strong> </p>\n<p>The first thing I think about in an actual interview is to ensure that the candidate is relaxed and comfortable, so I know they will be at their best. I also try and save the last 10 minutes for them to ask questions.</p>\n<p>The biggest change I\'ve made to my interview approach is to lean much more towards <a href="http://www.hr.az.gov/Employment/EMP_StructuredBehavioralInterview.asp">Structured Behavioral Questions</a> (SBQs) for the types of questions I ask. The defining characteristic of an SBQ is that it gets the interviewee talking about their <em>specific</em> experience and <em>actual</em> knowledge more than the hypothetical questions people often use. Here are some examples:</p>\n<p>Core Skills and Requirements:</p>\n<ul>\n<li><em>How would you analyze the market opportunity for a new product?</em></li>\n<li><em>How do you get a feature team from a feature idea to the spec complete milestone?</em></li>\n<li><em>What should a PM do during the coding milestone?</em></li>\n<li><em>How do you run an A/B test? What do you do if the results are ambiguous?</em></li>\n</ul>\n<p>Employee Experience: </p>\n<ul>\n<li><em>What is a difficult decision at work you\'ve had to make in the past year? How did you arrive at your decision? What was the result?</em></li>\n<li><em>Tell me about your contribution to this project on your resume</em></li>\n<li><em>Teach me about something you\'re an expert at</em></li>\n<li><em>Check here for a large <a href="http://recruitloop.com/blog/behavioural-interview-questions/">archive of questions</a></em></li>\n</ul>\n<p>Real Problems Faced by Your Team:</p>\n<ul>\n<li><em>There are two approaches we\'re considering for our go-to-market strategy: build our own channel, or partner with agencies. What do you think we should do?</em></li>\n<li><em>We have this feature that we\'ve built, but it doesn\'t have any user interaction. How can we measure the success of this feature?</em></li>\n<li><em>Here\'s a copy of our most recent experiment results. What do these results tell you? What should we do next? How would you communicate these to the team? To upper management?</em></li>\n</ul>\n<p>After asking these types of questions for a while, I\'ve started to bucket responses into high, medium, low, based on how deeply they were able to address the question.  This has made it much easier to compare candidates quality over time. </p>\n<p><strong>Making the Call</strong></p>\n<p>Generally, if I\'m on the fence about a candidate then I go with not hiring them. However, in the past couple years I\'ve started taking a chance with 10-20% of my open positions, when someone is culturally a good fit, sharp and highly motivated, but might not have the specific skills or experience I was looking for. These folks tend to take a bit more support early on (which is why I do this sparingly). But they have been some of my best hires, and a great way to find people in a highly competitive market.</p>\n<h2>Sell, Sell, Sell!</h2>\n<p>You\'ve found the most amazing candidate and you want to lock it up? But they  start dragging their feet, or they have competitive offers they want to consider - this is where the rest of the advice comes in big. I have found the best approach is to (1) Inspire them with the impact they could have on your team, and (2) Show them what a great place it is to work.</p>\n<p>Get on the phone with them and try to understand the real decisions they are making, and find out what is really important to them. Are they deciding between two locations? Or between companies, or teams or careers? Then taylor the sales strategy to that.</p>\n<ul>\n<li>Which senior manager would be the most effective in selling. Stop by to ask them to do the sales pitch. (I\'ve never had anyone say no, from GM, to VP to President)</li>\n<li>Invite the candidate to spend time with the team. We had a design review meeting thursday afternoons where the team got together to gave feedback on all the new feature designs. This was the perfect meeting to bring a candidate to: they were inspired by the cool new features, and how the team came together in a supportive way to give feedback.</li>\n<li>Send them the names of a couple people already on your team (or as close as you can) so they can ask some frank questions about your management style and team culture.</li>\n<li>Send them a few of the finest and most interesting deliverables created by your team, that align to growth areas they discussed. For example, many of the people I interviewed at Bing were excited to use more data and experimentation to design their features. So, I would send them a few of the most compelling experiment write-ups so they could see how much they would learn.</li>\n<li>Never oversell the role, and always be completely upfront about the challenges they will face. Great employees will be able to tell anyways, and many of them will be inspired by challenges they can overcome.</li>\n</ul>\n<p>Most hiring managers don\'t do any of these things. If you do, they will always remember it, even if they don\'t take this specific role. And you have a much better change of keeping in touch with them and rehiring in the long term.</p>\n<h2>Conclusion</h2>\n<p>At the end of the day, talent management is like playing Tetris in slow motion. You should be always looking for where people will have the best fit and move them into those places. And the blocks never stop falling. Even though your team may be full today, you will always need to be planning for inevitable attrition and investing in your network. Don\'t be afraid of losing your best people, because it <em>will</em> happen and it may just be a blessing in disguise. Just be prepared in advance to replace them and ensure they had a great experience on your team so your network will extend with them into their new role.</p>', 'title': 'Hiring: Lessons Learned at Microsoft'}
----------------
{'slug': '2007-01-02-mortgage-planning-tool', 'build_path': './build/articles/Finance/2007-01-02-mortgage-planning-tool.html', 'category': 'Finance', 'url': '/articles/Finance/2007-01-02-mortgage-planning-tool.html', 'date': datetime.datetime(2007, 1, 2, 0, 0), 'content_html': '<p>Here\'s an easy modeling tool for use with standard 30 year, fixed-rate \nmortgages. You can use it to do "what if" analysis for different real estate \nproperties you are considering, or evaluating the impact of making additional \npayments.</p>\n<p>Download file: <a href="/downloads/mortage-planning-tool.xls">\nMortgage Planning Tool (Excel 147kb)</a></p>\n<p><img alt="mortgage planning tool for excel" src="/images/articles/mortgage-planning-tool-excel-template.png" /></p>', 'title': 'Mortgage Planning Tool'}
----------------
{'slug': '2008-06-06-teched-developer-2008-advanced-seo-for-web-development', 'build_path': './build/articles/SEO/2008-06-06-teched-developer-2008-advanced-seo-for-web-development.html', 'category': 'SEO', 'url': '/articles/SEO/2008-06-06-teched-developer-2008-advanced-seo-for-web-development.html', 'date': datetime.datetime(2008, 6, 6, 0, 0), 'content_html': '<p>I would like to thank everyone for attending my session at this year’s <a href="http://www.microsoft.com/events/teched2008/developer/default.mspx">TechEd Developer</a> conference in Orlando, FL. Below I’ve included a link to my slides, please feel free to contact me if you have any questions. This is very similar to the <a href="http://nathanbuggia.com/post/Mix08-Presentation-Advanced-SEO-for-Developers.aspx">Mix08 Advanced SEO Presentation</a> I gave a few months ago at Mix08, except I expanded the “Diagnosing SEO issues within your site” section. For a deeper dive on the subject, check out the 3 hour <a href="http://janeandrobot.com/admin/Pages/web20presentations.html">SEO for Web Development</a> workshop <a href="http://www.ninebyblue.com/about/">Vanessa Fox</a> and I presented on this topic at Web 2.0.</p>\n<p>I also recommend you check out the <a href="http://webmaster.bing.com/">Bing Webmaster Center</a> for information about how Bing is crawling your site, news and discussion forums. Have I done enough shameless plugging yet?</p>\n<p><a href="/downloads/Advanced-SEO-Web-Development-TechEd-2008.pptx">Advanced SEO Web Development TechEd 2008.pptx (5.31 mb)</a></p>\n<iframe src="http://www.slideshare.net/slideshow/embed_code/451317" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen webkitallowfullscreen mozallowfullscreen> </iframe>\n<div style="margin-bottom:5px"> <strong> <a href="http://www.slideshare.net/nbuggia/advanced-seo-web-development-tech-ed-2008" title="Advanced Seo Web Development Tech Ed 2008" target="_blank">Advanced Seo Web Development Tech Ed 2008</a> </strong> from <strong><a href="http://www.slideshare.net/nbuggia" target="_blank">Nathan Buggia</a></strong> </div>', 'title': 'TechEd Developer 2008: Advanced SEO for Web Development'}
----------------
{'slug': '2008-10-18-web-2-0-expo-advanced-seo-for-developers', 'build_path': './build/articles/SEO/2008-10-18-web-2-0-expo-advanced-seo-for-developers.html', 'category': 'SEO', 'url': '/articles/SEO/2008-10-18-web-2-0-expo-advanced-seo-for-developers.html', 'date': datetime.datetime(2008, 10, 18, 0, 0), 'content_html': '<p>I would like to thank everyone for taking the time to check out my session at Web 2.0 Expo here in New York City. I\'ve posted my presentation and answers to all the online questions on the Webmaster Center Blog at Live Search.</p>\n<p><a href="https://blogs.bing.com/webmaster/2008/09/24/web-2-0-expo-seo-for-web-development-presentation">Q&amp;A from Bing Blog</a></p>\n<p><a href="/downloads/Web_20_NYC_2008.pptx">Presentation</a></p>', 'title': 'Web 2.0 Expo: Advanced SEO for Developers'}
----------------
{'slug': '2009-06-04-managing-search-engine-access-to-your-site', 'build_path': './build/articles/SEO/2009-06-04-managing-search-engine-access-to-your-site.html', 'category': 'SEO', 'url': '/articles/SEO/2009-06-04-managing-search-engine-access-to-your-site.html', 'date': datetime.datetime(2009, 6, 4, 0, 0), 'content_html': '<p>Note: Originally published on http://janeandrobot.com</p>\n<p>Controlling what content is blocked from being found in search engines is crucial for many websites. Fortunately, the major search engines and other well-behaved robots observe the <a href="http://www.robotstxt.org/" target="_blank">Robots Exclusion Protocol</a> (REP), which has evolved organically since the early 1990′s to provide a set of controls over what parts of a web site search engines robots can crawl and index.</p>\n<p>Article Sections:</p>\n<ul>\n    <li><a href="#Capabilities_of_the_REP">Capabilities of REP</a></li>\n    <li><a href="#Deciding_what_should_be_Public_vs._Private">Deciding What Should be Public vs. Private</a></li>\n    <li><a href="#Implementing_the_REP">Implementing the REP</a>\n<ul style="margin-bottom: 0pt;">\n    <li><a href="#Site_Level_Implementation_(Robots.txt)">Site Level</a></li>\n    <li><a href="#Page_Level_Implementation_(META_Tags)">Page Level (Meta Tags)</a></li>\n    <li><a href="#HTTP_Header_Implementation_(X-ROBOTS-Tag)">Page Level (HTTP Header)</a></li>\n    <li><a href="#Content_Level_Implementation">Content Level</a></li>\n</ul>\n</li>\n    <li><a href="#Common_implementation_mistakes">Common Mistakes</a></li>\n    <li><a href="#Testing_your_implementation_">Testing Your Implementation</a></li>\n    <li><a href="#removal">Removing Content From Search Engine Indices</a></li>\n    <li><a href="#Additional_Resources:_">Additional Resources</a></li>\n</ul>\n\n<h2><a name="Capabilities_of_the_REP"></a>Capabilities of the REP</h2>\n<p>The Robots Exclusion Protocol provides controls that can be applied at the site level (robots.txt), at the page level (META tag, or X-Robots-Tag), or at the HTML element level to control both the crawl of your site and the way it’s listed in the search engine results pages (SERPs). Below is a table listing the common scenarios, directives, and which search engines support them.</p>\n<table class="small">\n    <tr>\n    <th>Use Case</th>\n    <th class="center">Robots.txt</th>\n    <th class="center">META Tag<br/>X-Robots-Tag</th>\n    <th class="center">Other</td>\n    <th class="center">Supported By</th>\n    </tr>\n    <tr>\n    <td>Allow access to your content</td>\n    <td class="center">Allow</td>\n    <td class="center">FOLLOWINDEX</td>\n    <td class="center"></td>\n    <td class="center"><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=40364">Google</a>, <a href="http://blogs.msdn.com/webmaster/archive/2008/06/03/robots-exclusion-protocol-joining-together-to-provide-better-documentation.aspx">Microsoft</a></td>\n    </tr>\n    <tr>\n    <td>Disallow access to your content</td>\n    <td class="center">Disallow</td>\n    <td class="center">NOINDEXNOFOLLOW</td>\n    <td class="center"></td>\n    <td class="center"><a href="http://www.google.com/support/webmasters/bin/answer.py?hl=en&amp;answer=35303">Google</a>, <a href="http://blogs.msdn.com/webmaster/archive/2008/06/03/robots-exclusion-protocol-joining-together-to-provide-better-documentation.aspx">Microsoft</a></td>\n    </tr>\n    <tr>\n    <td>Disallow access to index images on the page</td>\n    <td class="center"></td>\n    <td class="center">NOIMAGEINDEX</td>\n    <td class="center"></td>\n    <td class="center"><a href="http://www.google.com/support/webmasters/bin/answer.py?hl=en&amp;answer=79892">Google</a></td>\n    </tr>\n    <tr>\n    <td>Disallow the display of a cached version of your content in the SERP</td>\n    <td class="center"></td>\n    <td class="center">NOARCHIVE</td>\n    <td class="center"></td>\n    <td class="center"><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=35306=">Google</a>, <a href="http://blogs.msdn.com/webmaster/archive/2008/06/03/robots-exclusion-protocol-joining-together-to-provide-better-documentation.aspx">Microsoft</a></td>\n    </tr>\n    <tr>\n    <td>Disallow the creation of a description for this content in the SERP</td>\n    <td class="center"></td>\n    <td class="center">NOSNIPPET</td>\n    <td class="center"></td>\n    <td class="center"><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=35304">Google</a>, <a href="http://blogs.msdn.com/webmaster/archive/2008/06/03/robots-exclusion-protocol-joining-together-to-provide-better-documentation.aspx">Microsoft</a></td>\n    </tr>\n    <tr>\n    <td>Disallow the translation of your content into other languages</td>\n    <td class="center"></td>\n    <td class="center">NOTRANSLATE</td>\n    <td class="center"></td>\n    <td class="center"><a href="http://www.google.com/help/faq_translation.html#donttrans">Google</a></td>\n    </tr>\n    <tr>\n    <td>Do not follow or give weight to links within this content</td>\n    <td class="center"></td>\n    <td class="center">NOFOLLOW</td>\n    <td class="center">rel=NOFOLLOW</td>\n    <td class="center"><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=96569">Google</a>, <a href="http://blogs.msdn.com/livesearch/archive/2005/01/18/nofollow_tags.aspx">Microsoft</a></td>\n    </tr>\n    <tr>\n    <td>Do not use the <a href="http://www.dmoz.org/" target="_blank">Open Directory Project</a> (ODP) to create descriptions for your content in the SERP</td>\n    <td class="center"></td>\n    <td class="center">NOODP</td>\n    <td class="center"></td>\n    <td class="center"><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=35264">Google</a>, <a href="http://blogs.msdn.com/webmaster/archive/2008/06/03/robots-exclusion-protocol-joining-together-to-provide-better-documentation.aspx">Microsoft</a></td>\n    </tr>\n    <tr>\n    <td>Stop indexing this content after a specific date</td>\n    <td class="center"></td>\n    <td class="center">UNAVAILABLE_AFTER</td>\n    <td class="center"></td>\n    <td class="center"><a href="http://googleblog.blogspot.com/2007/07/robots-exclusion-protocol-now-with-even.html">Google</a></td>\n    </tr>\n    <tr>\n    <td>Disallow the creation of enhanced captions</td>\n    <td class="center"></td>\n    <td class="center">NOPREVIEW</td>\n    <td class="center"></td>\n    <td class="center"><a href="http://bing.com/community">Microsoft</a></td>\n    </tr>\n    <tr>\n    <td>Specify a sitemap file or a sitemap index file</td>\n    <td class="center">Sitemap</td>\n    <td class="center"></td>\n    <td class="center"></td>\n    <td class="center"><a href="http://www.google.com/support/webmasters/bin/answer.py?hl=en&amp;answer=64748">Google</a>, <a href="http://blogs.msdn.com/livesearch/archive/2007/04/11/discovering-sitemaps.aspx">Microsoft</a></td>\n    </tr>\n    <tr>\n    <td>Specify how frequently a crawler may access your website</td>\n    <td class="center">Crawl-Delay</td>\n    <td class="center"></td>\n    <td class="center"><a href="http://google.com/webmaster">Google WMT</a></td>\n    <td class="center"><a href="http://blogs.msdn.com/webmaster/archive/2008/04/18/ramping-up-msnbot.aspx">Microsoft</a></td>\n    </tr>\n    <tr>\n    <td>Authenticate the identity of the crawler</td>\n    <td class="center"></td>\n    <td class="center"></td>\n    <td class="center">Reverse DNS Lookup</td>\n    <td class="center"><a href="http://googlewebmastercentral.blogspot.com/2006/09/how-to-verify-googlebot.html">Google</a>, <a href="http://blogs.msdn.com/livesearch/archive/2006/11/29/search-robots-in-disguise.aspx">Microsoft</a></td>\n    </tr>\n    <tr>\n    <td>Request removal of your content from the engine’s index</td>\n    <td class="center"></td>\n    <td class="center"></td>\n    <td class="center">\n        <a href="http://google.com/webmaster">Google WMT</a>, \n        <a href="http://webmaster.bing.com/">Microsoft WMT</a>\n    </td>\n    <td class="center"><a href="http://googlewebmastercentral.blogspot.com/2007/04/requesting-removal-of-content-from-our.html">Google</a>, <a href="http://webmaster.bing.com">Microsoft</a>\n    </td>\n    </tr>\n</table>\n\n<p>&nbsp;</p>\n<h2><a name="Deciding_what_should_be_Public_vs._Private"></a>Deciding What Should be Public vs. Private</h2>\n<p>One of the first steps in managing the robots is knowing what type of content should be public vs. private. Start with the assumption that by default, everything is public, then explicitly identify the items that are private.</p>\n<p>If you want search engines to access all the content on your site, you don’t need a robots.txt file at all. When a search engine tries to access the robots.txt file on your site and the server can’t return one (ideally by returning a 404 HTTP status code), the search engine treats this the same as a robots.txt file that allows access to everything.</p>\n<p>Every website and every business has a different set of needs, so there’s no blanket rule for what to make private, but some common elements may apply.</p>\n<ul>\n<li><strong>Private data</strong> – You may have content on your site that you don’t want to be searchable in search engines. For instance, you may have private user information (such as addresses) that you don’t want surfaced. For this type of content, you may want to use a more secure approach that keeps all visitors from the pages (such as password protection). However, some types of content are fine for visitor access, but not search engine access. For instance, you may run a discussion forum that is open for public viewing, but you may not want individual posts to appear in search results for forum member names.</li>\n<li><strong><a name="noncontent"></a>Non-content content</strong> – Some content, like <a href="/post/Effectively-Using-Images.aspx#noncontent">images used for navigation</a>, provides little value to searchers. It’s not harmful to include these items in search engine indices, but since search engines allocate limited bandwidth to crawl each site and limited space to store content from each site, it may make sense to block these items to help direct the bots to the content on your site that you do want indexed.</li>\n<li><strong>Printer-friendly pages</strong> – if you have specific pages (URLs) that are formatted for printing you may want to block them out to avoid duplicate content issues. The drawback to allowing the printer-friendly page to be indexed is that it could potentially be listed in the search results instead of the default version of the page, which wouldn’t provide an ideal user experience for a visitor coming to the site through search.</li>\n<li><strong>Affiliate links and advertising</strong> – If you include advertising on your site, you can keep search engine robots from following the links by redirecting them to a blocked page, then on to the destination page. (There are other methods for implementing advertising-based links as well.)</li>\n<li><strong>Landing pages</strong> – Your site may include multiple variations of entry pages used for advertising purposes. For instance, you may run AdWords campaigns that link to a particular version of a page based on the ad, or you may print different URLs for different print ad campaigns (either for tracking purposes or to provide a custom experience related to the ad). Since these pages are meant to be an extension of the ad, and are generally near duplicates of the default version of the page, you may want to block these landing pages from being indexed.</li>\n<li><strong>Experimental pages</strong> – As you try new ideas on your site (for instance, using A/B testing), you likely want to block all but the original page from being indexed during the experiment.</li>\n</ul>\n<h2><a name="Implementing_the_REP"></a>Implementing the REP</h2>\n<p>REP is flexible and can be implemented a number of ways. This flexibility lets you easily specify some policies for your entire site (or subdomain) and then enhance them more granularly at the page or link level as needed.</p>\n<h3><a name="Site_Level_Implementation_(Robots.txt)"></a>Site Level Implementation (Robots.txt)</h3>\n<p>Site wide directives are stored in a robots.txt file, which must be located in the root directory of each domain or sub-domain (e.g. <a href="/robots.txt">http://janeandrobot.com/robots.txt</a>.) Note that robots.txt files only apply to the hostname where they are placed, and do not apply to subdomains. So a robots.txt file located on <a href="http://microsoft.com/robots.txt">http://microsoft.com/robots.txt</a> will not apply to the MSDN subdomain <a href="http://msdn.microsoft.com/">http://msdn.microsoft.com</a>. However, the robots.txt file does apply to all subfolders and pages within the specified hostname.</p>\n<p>A robots.txt file is a UTF-8 encoded file that contains entries that consist of a user-agent line (that tells the search engine robot if the entry is directed at it) and one or more directives that specify content that the search engine robot is blocked from crawling or indexing. A simple robots.txt file is shown below.</p>\n<pre><code>User-agent: *\nDisallow: /private\n</code></pre>\n\n<p><code>user-agent:</code> – Specifies which robots the entry applies to.</p>\n<ul>\n<li>Set this to <code>*</code> to specify that this entry applies to all search engine robots.</li>\n<li>Set this to a specific robot name to provide instructions for just that robot. You can find a complete list of robot names at <a href="http://www.robotstxt.org">robotstxt.org</a>.</li>\n<li>If you direct an entry at a particular robot, then it obeys that entry <em>instead</em> of any entries defined for <code>user-agent: * </code> (rather than in addition to those entries).</li>\n</ul>\n<p>The major search engines have multiple robots that crawl the web for different types of content (such as images or mobile). They generally begin all robots with the same name so that if you block the major robot, all robots for that search engine are blocked as well. However, if you want to block only the more specific robot, you can block it directly and still allow web crawl access.</p>\n<ul>\n<li><strong><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=40364">Google</a></strong> – The primary search engine robot is Googlebot.</li>\n<li><strong><a href="http://help.yahoo.com/l/us/yahoo/search/webcrawler/slurp-02.html">Yahoo!</a></strong> – The primary search engine robot is Slurp.</li>\n<li><strong><a href="http://blogs.msdn.com/livesearch/archive/2006/11/29/search-robots-in-disguise.aspx">Bing</a></strong> – The primary search engine robots is MSNbot.</li>\n</ul>\n<p><code>Disallow: </code> - Specifies what content is blocked</p>\n<ul>\n<li>Must begin with a slash (<code>/</code> ).</li></li>\n<li>Blocks access to any URLs that begin with the characters after the <code>/</code>. For instance, <code>Disallow: /images</code> blocks access to <code>/images/</code> , <code>/images/image1.jpg</code>, and <code>/images10</code>.</li>\n</ul>\n<p>You can specify other rules for search engine robots in addition to the standard instructions that block access to content as noted in <a href="#other">other robot instructions</a>.</p>\n<p>Some things to note about robots.txt implementation:</p>\n<ul>\n<li>The major search engines support pattern matching using the asterisk character (*) for wildcard match and the dollar sign ($) for end of sequence matching as described below in <a href="#patterns">using pattern matching</a>.</li>\n<li>The robots.txt file is case sensitive, so <code>Disallow: /images </code> would block <code>http://www.example.com/images</code> but not <code>http://www.example.com/Images</code>.</li>\n<li>If conflicts exist in the file, the robot obeys the longest (and therefore generally more specific) line.</li>\n</ul>\n<h4>Basic Samples</h4>\n<p><strong>Block all robots</strong></p>\n<p>Useful when your site is in pre-launch development and isn’t ready for search traffic.</p>\n<pre><code># This keeps out all well-behaved robots.\n# Disallow: * is not valid.\nUser-agent: *\nDisallow: /\n</code></pre>\n\n<p><strong>Keep out all bots by default</strong></p>\n<p>Blocks all pages except those specified. Not recommended as is difficult to maintain and diagnose.</p>\n<pre><code># Stay out unless otherwise stated\nUser-agent: *\nDisallow: /\nAllow: /Public/\nAllow: /articles/\nAllow: /images/\n</code></pre>\n\n<p><strong>Block specific content</strong></p>\n<p>The most common usage of robots.txt.</p>\n<pre><code># Block access to the images folder\nUser-agent: *\nDisallow: /images/\n</code></pre>\n\n<p><a name="allow"></a>\n<strong>Allow specific content</strong></p>\n<p>Block a folder, but allow access to selected pages in that folder.</p>\n<pre><code># Block everything in the images folder\n# Except allow images/image1.jpg\nUser-agent: *\nDisallow: /images/\nAllow: /images/image1.jpg\n</code></pre>\n\n<p><strong>Allow specific robot</strong></p>\n<p>Block a class of robots (for instance, Googlebot), but allow a specific bot in that class (for instance, Googlebot-Mobile).</p>\n<pre><code># Block Googlebot access\n# Allow Googlebot-Mobile access\nUser-agent: Googlebot\nDisallow: /\nUser-agent: Googlebot-Mobile\nAllow: /\n</code></pre>\n\n<h4>Pattern Matching Examples</h4>\n<p>The major engines support two types of pattern matching.</p>\n<ul>\n<li><strong>*</strong>matches any sequence of characters</li>\n<li><strong>$</strong> matches the end of URL</li>\n</ul>\n<p><strong>Block access to URLs that contain a set of characters</strong></p>\n<p>Use the asterisk (*) to specify a wildcard.</p>\n<pre><code># Block access to all URLs that include an ampersand\nUser-agent: *\nDisallow: /*&amp;\n</code></pre>\n\n<p>This directive would block search engines from crawling <code>http://www.example.com/page1.asp?id=5&amp;sessionid=xyz</code>.</p>\n<p><strong>Block access to URLs that end with a set of characters</strong></p>\n<p>Use the dollar sign ($) to specify end of line.</p>\n<pre><code># Block access to all URLs that end in .cgi\nUser-agent: *\nDisallow: /*.cgi$\n</code></pre>\n\n<p>This directive would block search engines from crawling <code>http://www.example.com/script1.cgi</code> but not from crawling <code>http://www.example.com/script1.cgi?value=1</code>.</p>\n<p><strong>Selectively allow access to a URL that matches a blocked pattern</strong></p>\n<p>Use the <code>Allow</code> directive in conjunction with pattern matching for more complex implementations.</p>\n<pre><code># Block access to URLs that contain ?\n# Allow access to URLs that end in ?\nUser-agent: *\nDisallow: /*?\nAllow: /*?$\n</code></pre>\n\n<p>That directive blocks all URLs that contain <code>?</code> except those that end in <code>?</code> . In this example, the default version of the page will be indexable: <code>http://www.example.com/productlisting.aspx?</code></p>\n<p>Variations of the page will be blocked:\n* <code>http://www.example.com/productlisting.aspx?nav=price</code>\n* <code>http://www.example.com/productlisting.aspx?sort=alpha</code></p>\n<h4><a name="other"></a>Other robot instructions</h4></h4>\n<p><strong>Specify a Sitemap or Sitemap index file</strong></p>\n<p>If you’d like to provide search engines with a comprehensive list of your best URLs, you can provide one or more <a href="http://sitemaps.org" target="_blank">Sitemap</a> autodiscovery directives. Note, user-agent does not apply to this directive so you cannot use this to specify a Sitemap to some but not all search engines.</p>\n<pre><code>\n# Please take my sitemap and index everything!\nSitemap: <a href="http://janeandrobot.com/sitemap.axd">http://janeandrobot.com/sitemap.axd</a>\n</code></pre>\n\n<p><strong>Reduce the crawling load</strong></p>\n<p>This only works with Microsoft and Yahoo. For Google you’ll need to specify a slower crawling speed through their <a href="http://google.com/webmaster" target="_blank">Webmaster Tools</a>. Be careful when implementing this because if you slow down the crawl too much, robots won’t be able to get to all of your site and you may lose pages from the index.</p>\n<pre><code>\n# MSNBot, please wait 5 seconds in between visits\nUser-agent: msnbot\nCrawl-delay: 5\n\n# Yahoo\'s Slurp, please wait 12 seconds in between visits\nUser-agent: slurp\nCrawl-delay: 12\n</code></pre>\n\n<h3><a name="Page_Level_Implementation_(META_Tags)"></a>Page Level Implementation (META Tags)</h3>\n<p>The REP page-level directives allow you to refine the site wide policies on a page-by-page basis</p>\n<p><strong>Placing a meta tag on the page</strong></p>\n<p>Place the meta tag in the head tag. Each directive should be comma delimited inside the tag. E.g. &lt;meta name=”ROBOTS” content=”Directive1, Directive 2&gt;.</p>\n<pre><code><span class="kwrd">&lt;</span><span class="html">html</span><span class="kwrd">&gt;</span>\n<span class="kwrd"> &lt;</span><span class="html">head</span><span class="kwrd">&gt;</span>\n<span class="kwrd"> &lt;</span><span class="html">title</span><span class="kwrd">&gt;</span>Your title here<span class="kwrd">&lt;/</span><span class="html">title</span><span class="kwrd">&gt;</span>\n<span class="kwrd"> &lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="NOINDEX"</span><span class="kwrd">&gt;</span>\n<span class="kwrd"> &lt;/</span><span class="html">head</span><span class="kwrd">&gt;</span>\n<span class="kwrd"> &lt;</span><span class="html">body</span><span class="kwrd">&gt;</span>Your page here<span class="kwrd">&lt;/</span><span class="html">body</span><span class="kwrd">&gt;</span>\n<span class="kwrd">&lt;/</span><span class="html">html</span><span class="kwrd">&gt;</span>\n</code></pre>\n\n<p><strong>Targeting a specific search engine</strong></p>\n<p>Within the meta tag you can specify which search engine you would like to target, or you can target them all.</p>\n<pre><code><span class="rem">&lt;!-- Applies to All Robots --&gt;</span>\n<span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="NOINDEX"</span><span class="kwrd">&gt;</span>\n&nbsp;\n<span class="rem">&lt;!-- ONLY GoogleBot --&gt;</span>\n<span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="Googlebot"</span> <span class="attr">content</span><span class="kwrd">="NOINDEX"</span><span class="kwrd">&gt;</span>\n&nbsp;\n<span class="rem">&lt;!-- ONLY Slurp (Yahoo) --&gt;</span>\n<span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="Slurp"</span> <span class="attr">content</span><span class="kwrd">="NOINDEX"</span><span class="kwrd">&gt;</span>\n&nbsp;\n<span class="rem">&lt;!-- ONLY MSNBot (Microsoft) --&gt;</span>\n<span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="MSNBot"</span> <span class="attr">content</span><span class="kwrd">="NOINDEX"</span><span class="kwrd">&gt;</span>\n</code></pre>\n\n<p><em>Control how your listings appear</em> – there are a set of options you can use to determine how your site will show up on the SERP. You can exert some control over how the description is created, and remove the “Cached page” link.</p>\n<p><img class="aligncenter size-full wp-image-85" style="border: black 1px solid;" title="example-serp" alt="example-serp" src="http://janeandrobot.com/wp-content/uploads/2008/06/example-serp.gif" /></p>\n<pre><code><span class="rem">&lt;!-- Do not show a description for this page --&gt;</span>\n<span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="NOSNIPPET"</span><span class="kwrd">&gt;</span>\n&nbsp;\n<span class="rem">&lt;!-- Do not use http://dmoz.org to create a description --&gt;</span>\n<span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="NOODP"</span><span class="kwrd">&gt;</span>\n&nbsp;\n<span class="rem">&lt;!-- Do not present a cached version of the document in a search result --&gt;</span>\n<span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="NOARCHIVE"</span><span class="kwrd">&gt;</span>\n</code></pre>\n\n<p><strong>Using other directives</strong></p>\n<p>Other meta robots directives are shown below.</p>\n<pre><code><span class="rem">&lt;!-- Do not trust links on this page, could be user generated content (UCG) --&gt;</span>\n<span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="NOFOLLOW"</span><span class="kwrd">&gt;</span>\n<span class="rem">&lt;!-- Do not index this page --&gt;</span>\n<span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="NOINDEX"</span><span class="kwrd">&gt;</span>\n&nbsp;\n<span class="rem">&lt;!-- Do not index any images on this page (will still index the if they are linked</span>\n<span class="rem"> elsewhere) Better to use Robots.txt if you really want them safe.</span>\n<span class="rem"> This is a Google Only tag. --&gt;</span>\n<span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="GOOGLEBOT"</span> <span class="attr">content</span><span class="kwrd">="NOIMAGEINDEX"</span><span class="kwrd">&gt;</span>\n&nbsp;\n<span class="rem">&lt;!-- Do not translate this page into other languages--&gt;</span>\n<span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="NOTRANSLATE"</span><span class="kwrd">&gt;</span>\n&nbsp;\n<span class="rem">&lt;!-- NOT RECOMMENDED, there really isn\'t much point in using these --&gt;</span>\n<span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOTS"</span> <span class="attr">content</span><span class="kwrd">="FOLLOW"</span><span class="kwrd">&gt;</span>\n<span class="kwrd">&lt;</span><span class="html">meta</span> <span class="attr">name</span><span class="kwrd">="ROBOT"</span> <span class="attr">content</span><span class="kwrd">="UNAVAILABLE_AFTER"</span><span class="kwrd">&gt;</span>\n</code></pre>\n\n<h3><a name="HTTP_Header_Implementation_(X-ROBOTS-Tag)"></a>HTTP Header Implementation (X-ROBOTS-Tag)</h3>\n<p>Allows developers to specify page-level REP directives for non text/html content types like PDF, DOC, PPT, or dynamically generated images.</p>\n<p><strong>Using the X-Robots-Tag</strong></p>\n<p>Use the X-Robots-Tag, simply add it to your header as shown below. To specify multiple directives you can either comma delimit them, or add them as separate header items.</p>\n<pre><code>HTTP/1.x 200 OK\nCache-Control: private\nContent-Length: 2199552\nContent-Type: application/octet-stream\nServer: Microsoft-IIS/7.0\ncontent-disposition: inline; filename=01 - The truth about SEO.ppt\n<strong>X-Robots-Tag: noindex, nosnippet</strong>\nX-Powered-By: ASP.NET\nDate: Sun, 01 Jun 2008 19:25:47 GMT\n</code></pre>\n\n<p>The X-Robots-Tag directive supports most of the same directives as the meta tag. The only limitation with this method over the meta tag implementation is that there is no way to target a specific robot – though that probably isn’t a big deal for most use cases.</p>\n<ul>\n<li><code>X-Robots-Tag: noindex</code></li>\n<li><code>X-Robots-Tag: nosnippet</code></li>\n<li><code>X-Robots-Tag: notranslate</code></li>\n<li><code>X-Robots-Tag: noarchive</code></li>\n<li><code>X-Robots-Tag: unavailable_after: 7 Jul 2007 16:30:00 GMT</code></li>\n</ul>\n<h3><a name="Content_Level_Implementation"></a>Content Level Implementation</h3>\n<p>You can further refine your site level and page level directives within several content tags.</p>\n<p>Each anchor tag (link) can be modified to tell search engines that you do not trust where this URL is pointing to. This is typically used for links within user generated content (UCG) like wikis, blog comments, reviews and other community sites.</p>\n<pre><code>&lt;a href="#" rel="NOFOLLOW"&gt;My Hyperlink&lt;/a&gt;\n</code></pre>\n\n<p>Also, in Yahoo Search you can specify which &lt;div&gt; elements on a page you would not like indexed using the <code>class=robots-nocontent</code> attribute. However, we don’t highly recommend using this tag because it is not supported in any other engine, making it not super-useful.</p>\n<pre><code>&lt;div class="robots-nocontent"&gt;\nNo content for you! (or at least Yahoo!)\n&lt;/div&gt;\n</code></pre>\n\n<h2><a name="Common_implementation_mistakes"></a>Common Mistakes</h2>\n<p>While implementing the REP is generally straight-forward, there are a few common mistakes.</p>\n<p><strong>GoogleBot follows the most specific directive, ignoring all others</strong></p>\n<p>In the robots.txt file, if you specify a section for all user-agents (<code>user-agent: *</code> ) and also declare a section for Googlebot (<code>user-agent: Googlebot</code> ), Google will disregard all sections in the robots.txt file except the Googlebot section. This could potentially leave you exposing much more content to Google that you might have thought.</p>\n<pre><code># This keeps out all well-behaved robots\nUser-agent: *\nDisallow: /\n\n# This looks like it is giving Google access to only this directory, but since it is a\n# GoogleBot specific section, Google will disregard the previous section\n# and access the whole site.\nUser-agent: Googlebot\nAllow: /Content_For_Google/\n</code></pre>\n\n<p><strong>NOFOLLOW will most likely not prevent indexing</strong></p>\n<p>If you use <code>NOFOLLOW</code> at either the page or the link level, it is still possible for the links from the page to be indexed because the search engine may have found a reference to them from another source. Another note, using <code>rel="NOFOLLOW"</code> within your anchor text is still perceived as a recommendation by the search engines, not a command.To ensure that content is not indexed, either use the <code>Disallow</code> directive at the site level, or use <code>NOINDEX</code> at the page level.</p>\n<p><strong>Directives that are not recommended</strong></p>\n<p>Directives in the REP are all about exceptions, by default the robots assume they can crawl your whole site. Therefore, you do not need to explicitly use the <code>FOLLOW</code> and <code>INDEX</code> directives as they will not be taken into account by the search engines. It sounds silly but I’ve seen a few sites that have implemented these on every page and every link.Another directive that is not recommended is the <code>NOCACHE</code> directive. This was created by Microsoft, and is synonymous with <code>NOARCHIVE</code> . While they will most likely always continue to support the directive, it is better to use <code>NOARCHIVE</code> so it will work on all the search engines.</p>\n<p><strong>Be cognizant of case</strong></p>\n<p>When referencing files and URLs in the Robots.txt file, use a defensive approace to URL case, as the major engines do not handle it the same way. (e.g. /Files does not always equal /files).</p>\n<h2><a name="Testing_your_implementation_"></a>Testing Your Implementation</h2>\n<p>As you’re implementing your REP design, you should test it both before you deploy it and after. The easiest way to test this is to use the robots validator in either Google or Microsoft’s Webmaster Tools. These tools are generally good enough test beds for most folks, however advanced developers (or paranoid ones with critical business requirements) will want to definitively know what the robots are doing, not simply rely on what the robots say they are doing. These folks will want to look at their tools as well look at their server logs.</p>\n<p>In addition to using validation tools, reporting tools from the search engines on what they couldn’t acces, and looking at logs data to see what the search engine robots are crawling, you should check the search engine results to see if any pages you are intending to block are being indexed. If they are, use the methods described in this section to ensure you are blocking them correctly and <a href="#removal">use the search engine tools to request that the pages be removed</a>.</p>\n<p><strong>When Blocked Content Appears to be Indexed</strong></p>\n<p>If search engines are blocked from crawling pages, they may still index the URL if the robot finds a link to that URL on a page that isn’t blocked. The listing may display the URL only, such as shown below.</p>\n<p><img class="aligncenter size-full wp-image-84" title="urlonly" alt="urlonly" src="http://janeandrobot.com/wp-content/uploads/2008/06/urlonly.gif" /></p>\n<p>Or, it may include a title and in some instances, a description. This makes it appear as though the search engine robot is disregarding the directive that blocks access to the page, but the search engine is in fact obeying the directive not to crawl the page and is using anchor text from the link to that page and descriptive details from either the page that contains the link or a source such as the <a href="http://www.dmoz.org">Open Directory Project</a>.</p>\n<p>For more details, see:</p>\n<ul>\n<li><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=35667">Google: partially indexed page</a></li>\n<li><a href="http://help.yahoo.com/l/us/yahoo/search/webcrawler/slurp-01.html">Yahoo!: thin documents</a></li>\n</ul>\n<h3><a name="The_Easy_Way_"></a>The Easy Way</h3>\n<p>Both Google and Microsoft provide some tools as part of their Webmaster Centers to help you verify if you’ve configured your REP the way you expect. Let’s start with Google’s tools:</p>\n<p>The first thing you should check are the list of URLs that Google has seen from your website and not indexed due to the REP. Note you can also download the list and filter, sort, and have-your-way-with-it in Excel.</p>\n<p><img class="aligncenter size-full wp-image-83" style="border: black 1px solid;" title="webmaster-robotstxt-blocked1" alt="webmaster-robotstxt-blocked1" src="http://janeandrobot.com/wp-content/uploads/2008/06/webmaster-robotstxt-blocked1.gif" /></p>\n<p>The next step is to use their interactive robots.txt tool to analyze your rules and test specific URLs for blockage. When you pull up the tool they already should have it pre-populated with the robots.txt file they have on file from the last time they crawled. You can input a list of URLs you’d like to check below, select the user-agent you’d like to check against and the tool will tell you if they are blocked or not. You can also use the tool to test changes to your robots.txt file to see how Google would interpret things.</p>\n<p><img class="aligncenter size-full wp-image-81" style="border: black 1px solid;" title="google-analyze-robotstxt" alt="google-analyze-robotstxt" src="http://janeandrobot.com/wp-content/uploads/2008/06/google-analyze-robotstxt.jpg" /></p>\n<p>Microsoft has a similar tool in their <a href="http://webmaster.bing.com/">Webmaster Center</a> that will validate a robots.txt file against the standard that MSNBot supports. To use the tool, simply log in copy &amp; paste your robots.txt file into the top field and select <strong>Validate</strong>. A list of all detectable issues are displayed in the bottom box.</p>\n<p><img class="aligncenter size-full wp-image-79" style="border: black 1px solid;" title="microsoft-robotstxt-validat" alt="microsoft-robotstxt-validat" src="http://janeandrobot.com/wp-content/uploads/2008/06/microsoft-robotstxt-validat.jpg" /></p>\n<h3><a name="The_Hard_Way_(More_Accurate)"></a>The Hard Way</h3>\n<p><strong>More Accurate Views of Robot Access Through Your Logs</strong></p>\n<p>If you have a specific business need to ensure that the robots are following your rules, (or you’re just paranoid) then you should not simply rely on the tools they provide to test compliance. You’re going to need to go straight to the horse’s mouth and analyze your web server logs to see exactly what they are doing. There is no one easy tool for doing this, you’ll likely have to use an existing tool like one of these (<a href="http://www.microsoft.com/downloads/details.aspx?FamilyID=890cd06b-abf8-4c25-91b2-f8d975cf8c07">Microsoft HTTP Log Parser</a>) or write your own. It isn’t difficult, it will simply take some time to implement. A useful reference for this is a list of all the robot <a href="http://www.robotstxt.org/db.html">user agents</a>, and more complete list of bots from <a href="http://www.google.com/support/webmasters/bin/answer.py?answer=40364">Google</a>, and <a href="http://blogs.msdn.com/livesearch/archive/2006/11/29/search-robots-in-disguise.aspx">Microsoft</a>.</p>\n<p><strong><a name="verify"></a>Verifying Robot Identity</strong></p>\n<p>Another thing you’ll likely want to consider in this endeavor is to validate that the robots are who they actually say they are. Google, Yahoo and Microsoft all support <a href="http://en.wikipedia.org/wiki/Reverse_DNS_lookup">Reverse DNS authentication</a> of their robots. The process is pretty simple and described here by <a href="http://googlewebmastercentral.blogspot.com/2006/09/how-to-verify-googlebot.html">Google</a>, <a href="http://www.ysearchblog.com/archives/000460.html">Yahoo </a>and <a href="http://blogs.msdn.com/livesearch/archive/2006/11/29/search-robots-in-disguise.aspx">Microsoft</a>, essentially you simply find out what range their robot’s DNS is hosted in, and use that in your tool. This way, if the address changes (which it will), you don’t need to update your code.</p>\n<p>Should you find any issues, where one of the robots are not minding the REP, or are misbehaving in some other way, you can always communicate directly with each engine through one of their forums:</p>\n<ul>\n<li><a href="http://groups.google.com/group/Google_Webmaster_Help-Indexing/topics">Google Crawling, Indexing and Ranking Forum</a></li>\n<li><a href="http://help.yahoo.com/l/us/yahoo/search/search_support.html">Yahoo Crawler Feedback Form</a></li>\n<li><a href="http://forums.microsoft.com/webmaster/ShowForum.aspx?ForumID=1984&amp;SiteID=79">Microsoft Crawler Error and Feedback Forum</a></li>\n</ul>\n<h2><a name="removal"></a>Removing Content From Search Engine Indices</h2>\n<p>If you find that you haven’t implemented the techniques described here correctly and private content from your site is indexed, each of the major search engines has methods available for requesting that it be removed. For more information, see:</p>\n<ul>\n<li><a href="http://googlewebmastercentral.blogspot.com/2007/04/requesting-removal-of-content-from-our.html">Google: Requesting removal of content from our index</a></li>\n<li><a href="http://help.yahoo.com/l/us/yahoo/search/siteexplorer/delete/">Yahoo!: Deleting URLs</a></li>\n<li><a href="https://support.bing.com/eform.aspx?productKey=wlsearch&amp;page=wlsupport_home_options_form_byemail&amp;ct=eformts">Bing: Requesting content removal</a></li>\n</ul>\n<h2><a name="Additional_Resources:_"></a>Additional Resources:</h2>\n<ul>\n<li>Google<ul>\n<li><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=40362">How to create a robots.txt file</a></li>\n<li><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=40364">Descriptions of each user-agent that Google uses</a></li>\n<li><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=40367">How to use pattern matching</a></li>\n<li><a href="http://www.google.com/support/webmasters/bin/answer.py?answer=40368">How often we recrawl your robots.txt file</a></li>\n<li><a href="http://googlewebmastercentral.blogspot.com/2006/08/all-about-googlebot.html">All about Googlebot</a></li>\n</ul>\n</li>\n<li>Microsoft Bing<ul>\n<li><a href="http://blogs.msdn.com/livesearch/archive/2006/11/29/search-robots-in-disguise.aspx">Search robots in disguise</a></li>\n</ul>\n</li>\n<li>Other resources<ul>\n<li><a href="http://searchengineland.com/070305-204850.php">Search Engine Land: Meta Robots Tag 101</a></li>\n<li><a href="http://searchengineland.com/080603-121100.php">Search Engine Land: Yahoo!, Microsoft, Google Clarify Robots.txt Support</a></li>\n<li><a href="http://searchengineland.com/070417-213813.php">Search Engine Land: URL Removal Options</a></li>\n<li><a href="http://www.robotstxt.org/">robotstxt.org</a></li>\n<li><a href="http://en.wikipedia.org/wiki/Robots.txt">Wikipedia: Robots Exclusion Standard</a></li>\n</ul>\n</li>\n</ul>\n<h3>Revision History</h3>\n<ul>\n<li>02/12/2009 – Google, Yahoo and Microsoft make a joint announcement of the rel=’Canonical’ tag to make it easier for publishers to specify the canonical URLs.</li>\n<li>06/04/2009 – Added NOPREVIEW tag announced this week by Microsoft. Used to disable the ‘hover preview’ feature on their SERP.</li>\n<li>03/13/2013 - Removed Yahoo references because they are now powered by Microsoft, and renamed Live Search to Bing</li>\n</ul>', 'title': 'Managing Search Engine Access to Your Site'}
----------------
Rendered 24 articles in 5 categories and 1 custom pages.
